.. include:: preamble.rst

***************************************************
Objectives and constraints in optimization models
***************************************************

============================================
Feasibility problems (no objective function)
============================================

Sometimes it is not required to optimize any objective, but
it is sufficient to check if a set of constraints admits a feasible
solution. It is in particular required that a feasible solution is
returned, or, otherwise, a proof is given that the feasible set is empty.

As an example, in a scrap blending example it might be useful to know
if it is possible
to get a blend with
given chemical characteristics.
An easy way to cast a feasibility problem into an optimization one is
to add an objective function which is constantly equal to 0:

.. math::
   :nowrap:

   \begin{align*}
   \min 0^T \var{x} & \\
   A \var{x} & = b \\
   \var{x} & \geq 0
   \end{align*}


This problem admits optimum (with value 0) if and
only if the original problem is feasible. However 
if the problem is not feasible, there would be no cues on
the reason for infeasibility. By this, we mean an indication of a set
of constraints which could not be satisfied. For example, in a blend problem,
we might have required to small a percentage of Iron.

It might be useful in these cases to model an auxiliary problem (so
called *Phase I* optimization problem) whose aim is exactly that of
checking feasibility. By the way, this auxiliary problem is very often
automatic formulated and solved by optimization algorithms when an
initial feasible solution is not readily available.

Consider a generic feasibility problem (here we present the case of a
linear system of equations and inequalities, but some generalizations
are possible):

.. math::
   :nowrap:

   \begin{align*}
   A \var{x} & = b\\
   \var{x} & \geq 0 
   \end{align*}

It is assumed, without loss of generality, that :math:`b \geq 0`. If
this were not the case, a simple multiplication of both sides of the
corresponding equation by -1 would restore this assumption.
This problem can be converted into an optimization one through the
insertion of a set of non negative slack variables whose sum is
penalized in the objective function:

.. math::
   :nowrap:

   \begin{align*}
   \min \mathbf{1}^T \var{s}  & \\
   A \var{x} + \var{s} & = b\\
   \var{x} & \geq 0  \\
   \var{s} & \geq 0 
   \end{align*}

It can be proven that this linear optimization problem  always has an
optimal solution. And also that the objective function is equal to
zero if and only if the original set of constraints is feasible. Thus,
if the optimization returns a solution with value 0, the
:math:`\var{x}` vector   at the optimal solution is an example of a
feasible solution. Otherwise, if the optimum value is positive, it can
be proven that the original system is not feasible. Non zero values of
the slack variables in the optimal solution might be interpreted as a
possible cause of infeasibility, even if their interpretation is far
from being easy. A problem connected with this situation consists in
finding a minim  


====================
Multiple Objectives
====================

At the opposite extreme of the
previous case (no objectives) we analyze here  the case of
optimization problems in which 
more then one  criteria has to be  "optimized". From a formal point of view, an
optimization problem with multiple objectives is not defined. The reason
is related to the fact that  Euclidean spaces
whose dimension is greater  than 1 lack a total ordering and,
therefore, it is in general impossible to
compare different feasible solutions with respect to multiple
optimization criteria.

Consider, as an example, an assignment problem in which there are two
different criteria on which matching decision should be taken: the
preferences expressed by each person on the different tasks, but also
a cost which, e.g., might be the time needed to train each person for
each of the available duties. An example of simplified data is
contained in the following code fragment:


.. tabs::
   
   .. group-tab:: AMPL
      .. code-block:: ampl
      .. literalinclude:: AMPL/biobj-assignment.dat
	 :caption: biobj-assignment.dat
      
   .. group-tab:: Julia
      .. code-block:: Julia
      .. literalinclude:: Julia/TBD.jl
	 :caption: biobj-assignment.dat


In this datafile there are two cost matrices. The first one refers to
preferences: worker A prefers Job3, while the second choice is
Job7 and the least desirable one is  Job5.

The second cost matrix reports training time. In order to train A to
perform Job3 4 days are necessary, while 5 would be required for Job7;
training A for Job5 however would require only 3 days.

So it is evident that the two objectives, minimizing the preference
total index and minimizing the total training time are partially
conflicting.
In fact, optimizing the first objective, the following assignment is
obtained:

.. math::
   :nowrap:

   \begin{align*}
   \begin{array}{cccc}
   Person & Job & Pref & Cost \\ \hline
   A & Job3 & 1 & 4\\
   B & Job5 & 1 & 3\\
   C & Job1 & 1 & 2\\
   D & Job2 & 1 & 3\\
   E & Job4 & 1 & 4\\
   F & Job6 & 2 & 4\\
   G & Job7 & 1 & 5 \\ \hline
   Total&   & 8 &  25
   \\ \hline
   \end{array}
   \end{align*}

If we optimize instead the total training time, the following is
obtained:


.. math::
   :nowrap:

   \begin{align*}
   \begin{array}{cccc}
   Person & Job & Pref & Cost \\ \hline
   A & Job6 & 6 & 2\\
   B & Job2 & 4 & 1\\
   C & Job7 & 7 & 1\\
   D & Job4 & 3 & 1\\
   E & Job5 & 5 & 3\\
   F & Job3 & 7 & 2\\
   G & Job1 & 3 & 1 \\ \hline
   Total&   & 35 &  11
   \\ \hline
   \end{array}
   \end{align*}

If we consider both criteria, then it is not possible to choose
between these two options, although they are quite radically different
one from the other. 
Even without resorting to these extreme solutions, in some cases it is
not possible to decide which one is preferable. If we assigned jobs in
the  order 3, 2, 6, 4, 5, 7, 1 we would obtain a cost of assignment 21
and a training time 14: neither preferable not discardable with
respect to the others. However, if the assignment were, in the order,
7, 4, 6, 5, 1, 2, 3, we would obtain a cost of assignment 25 and a
training time 21: both costs are worse then the above solution. Thus
we can, without any doubt, avoid taking this solution into
consideration, as it is less appreciated and also requires more
training time with respect to another feasible assignment.


The presence of many objectives is quite 
frequent in the applications. For example, in production problems,  it
is often required to plan  production
at the least possible cost while maximizing
a quality index. In environmental management problems it is
frequent the case in which two conflicting objectives must be optimized:
an economic objective corresponding to
cost minimization and an environmental one, such as, for example, risk minimization.
(e.g., for the  transport of dangerous and polluting substances).

We can formalize the ideas behind this example as follows. Consider
the multiobjective problem

.. math::
   :nowrap: 
      
   \begin{align*} 
   \min \left \{f_{1} (\var{x}), f_{2} (\var{x}),
   \ldots, f_{k} ( \var{x}) \right \}  \\ 
   \var{x} \in X \subseteq \R^n
   \end{align*}
   
where we would like to simultaneously minimize more than one function 
:math:`f_1, f_2, \dots, f_k`.
      
It is reasonable to limit the search of solutions to :index:`non dominated`
or :index:`Pareto` ones. We say
that a feasible solution :math:`\var{y}`   is (weakly) *dominated*
by another feasible solution :math:`\var{x}`, if

.. math::
   :nowrap:
      
   \begin{align*}
   f_{j} (\var{x}) & \leq f_{j} (\var{y}) & \forall \, j \in 1,k
   \end{align*}
 
The solution :math:`\var{y}` is (strongly)  *dominated* by
:math:`\var{x}` if, besides being weakly dominated,  there exists
at least one index :math:`\bar{\jmath}` such that

.. math::
   :nowrap:
      
   \begin{align*}
   f_{j} (\var{x}) & \leq f_{j} (\var{y}) & \forall \, j \in 1,k \\
   f_{\bar{\jmath}} (\var{x}) & <f_{\bar{\jmath}} (\var{y})
   \end{align*}
   
A solution is *Pareto optimal*  if it not dominated by others, i.e., if no other feasible solution
exists for which every objective is at least as good as  this one,
and, in at least one case, strictly better.
 
The Pareto condition can be graphically represented in the
case of problems with 2 objectives.
Consider the following elementary bi-objective problem:

.. math::
   :nowrap:

   \begin{align*}
   \min_{\var{x},\var{y},\var{z}}& \{-2\var{x}+\var{y}+\var{z},
   \var{x}-2\var{y}+2\var{z}\} \\
   \var{x}+\var{y}-\var{z} & \leq 1 \\
   \var{x}-\var{y}+\var{z} & \leq 1 \\
   -\var{x}+\var{y}+\var{z} & \leq 1 \\
   \var{x},\var{y},\var{z} & \geq 0
   \end{align*}

whose vertices are :math:`(0,0,0), (1,0,0), (0,1,0), (0,0,1), (1,1,1)` and
whose feasible set can be represented as follows:

.. tikz::

   \begin{tikzpicture}[scale=2]
   \draw[->] (0,0) -- (1.5,0,0) node[right] {$x$};
   \draw[->] (0,0) -- (0,1.5,0) node[above] {$y$};
   \draw[->] (0,0) -- (0,0,1.5) node[below left] {$z$};
   \coordinate [label=below:A] (A) at (1,0,0);
   \coordinate [label=left:B] (B) at (0,1,0);
   \coordinate [label=left:C] (C) at (0,0,1);
   \coordinate [label=right:D] (D) at (1,1,1);
   \coordinate [label=left:O] (O) at (0,0,0);
   \foreach \i in {A,B,C,D}
   \draw[dashed] (0,0)--(\i);
   \draw[-, fill=green!30, opacity=.1] (O)--(A)--(B)--cycle;
   \draw[-, fill=green!30, opacity=.1] (O)--(A)--(C)--cycle;
   \draw[-, fill=green!30, opacity=.1] (O)--(B)--(C)--cycle;
   \draw[-, fill=red!30, opacity=.5] (A)--(D)--(B)--cycle;
   \draw[-, fill=blue!30, opacity=.5] (A) --(D)--(C)--cycle;
   \draw[-, fill=purple!30, opacity=.5] (B)--(D)--(C)--cycle;
   \foreach \i in {A,B,C,D,O} \fill (\i) circle[radius=1pt];
   \end{tikzpicture}

Mapping each feasible solution to the 2-dimensional space of the two
objective functions, the following set of objective values is
obtained:
   
.. tikz::

   \begin{tikzpicture}
   \draw[help lines] (-3,-4) grid (3,3);
   \draw[thick,->] (-3.2,0) -- (3.2,0) node[right] {$f_1(\cdot)$};
   \draw[thick,->] (0,-4.2) -- (0,3.2) node[above] {$f_2(\cdot)$};
   \coordinate [label=below right:O'] (o) at (0,0);
   \coordinate [label=above left:A'] (a) at (-2,1);
   \coordinate [label=above right:B'] (b) at (1,-2);
   \coordinate [label=above left:C'] (c) at (1,2);
   \coordinate [label=below left:D'] (d) at (0,1);
   \foreach \i in {a,b,c,d,o} \fill (\i) circle[radius=2pt];
   \draw[-, fill=orange!30, opacity=.5] (a)--(c)--(b)--cycle;
   \end{tikzpicture}

It might be proven that when both the objectives and the constraints
are linear, then the image of the feasible set in the space of
objectives is a polyhedron. Besides, the vertices of this polyhedron
are the images of some of the vertices of the feasible set.
From this picture it is immediately seen that vertex :math:`C` is not Pareto
optimal, as it is, as an example, dominated by each of the remaining
vertices. However, also vertex  :math:`O` is not Pareto, although no
other vertex dominates it; it is in fact dominated, e.g., by point
:math:`(1/2,1/2,0)`, whose objective function values are
:math:`(-1/2,-1/2)`. Graphically, in the space of objectives, Pareto
solutions are those feasible points whose image is
characterized by the absence of other image points in the bottom-left
quadrant:

.. tikz::

   \begin{tikzpicture}
   \draw[help lines] (-3,-4) grid (3,3);
   \draw[thick,->] (-3.2,0) -- (3.2,0) node[right] {$f_1(\cdot)$};
   \draw[thick,->] (0,-4.2) -- (0,3.2) node[above] {$f_2(\cdot)$};
   \coordinate [label=below right:O'] (o) at (0,0);
   \coordinate [label=above left:A'] (a) at (-2,1);
   \coordinate [label=above right:B'] (b) at (1,-2);
   \coordinate [label=above left:C'] (c) at (1,2);
   \coordinate [label=below left:D'] (d) at (0,1);
   \foreach \i in {a,b,c,d,o} \fill (\i) circle[radius=2pt];
   \draw[-, fill=orange!30, opacity=.5] (a)--(c)--(b)--cycle;
   \draw[fill=yellow!60,opacity=0.5] (-3,-.5)--(-.5,-.5)--(-.5,-4);
   \draw[very thick] (a) -- (b);
   \end{tikzpicture}

From this it can be readily understood that the image in the space of
objectives of Pareto solutions is a subset of the frontier, called
:index:`efficient frontier`. In the example above the efficient
frontier is outlined in bold and the set of Pareto optima is the edge
:math:`(A,B)` of the feasible set.
	    
Many software tools exist for solving multi-objective problems,
frequently they are capable of generating, sometimes in a partially
interactive way, some or even all Pareto solutions. 

A simple way to generate solutions to multiobjective problems consists
in reducing the problem to one with a 
single objective.
 
A first possibility, useful only at an initial stage of the analysis,
consists in optimizing each of the objectives in turn, disregarding
all of the others; this 
optimization phase consists of  an exploration of the  most extreme
solutions. In the example above, :math:`A` and :math:`B` minimize,
respectively, the first and the second objective. In the assignment
example before,  the two extreme solutions correspond one to an
assignment value of 8 and training time 25, while the other gives an
assignment index 35 with a training time 11. These solutions are
usually Pareto (unless a different solution, with the same value of
the optimized objective exists for which a better solution exists with
respect to the remaining ones). 

Usually  these solutions are not satisfactory, since they completely neglect
the presence of multiple objectives. In problems like this one,
characterized by linear, and thus convex, constraints, any convex
combination of feasible solution is feasible. Thus it is possible to
generate a family of solutions by combining Pareto optimal ones. As an
example, taking a convex combination of the two extreme solutions in
the geometrical example above we obtain the set

.. math::
   :nowrap:

   \begin{align*}
   (\lambda,(1-\lambda,0)) & & \lambda \in [0,1]
   \end{align*}

which corresponds to edge :math:`(A,B)` and, in this case, is the
set of Pareto optimal solutions. The situation is slightly different
in the assignment example, as the feasible set is no more convex: in
fact, although integrality is granted when optimizing a linear
objective function, it must be recalled that the feasible set is
composed of binary solution and, thus, a linear combination of
feasible solution is not, in general, feasible.

A second possibility to tackle multiobjective problems, although
related to the previous one  but somewhat  less radical, consists
in optimizing a single objective in turn, as before. However, all
other objectives  are included as  constraints. In particular, an objetive to be minimized
becomes a :math:`\leq` constraint; the right hand side
might be chosen based on the results obtained in the
optimization of that single objective.

.. math::
   :nowrap:
      
   \begin{align*}
   \min f_{h} (\var{x}) & \\
   f_{j} (\var{x}) & \leq \beta_{j} & \forall \, j \neq h \\
   \var{x} & \in X
   \end{align*}
 
In the geometrical example given above, considering that the minimum
value for the second objective is -2, solving:

.. math::
   :nowrap:

   \begin{align*}
   \min_{\var{x},\var{y},\var{z}}& -2\var{x}+\var{y}+\var{z} \\
   \var{x}+\var{y}-\var{z} & \leq 1 \\
   \var{x}-\var{y}+\var{z} & \leq 1 \\
   -\var{x}+\var{y}+\var{z} & \leq 1 \\
   \var{x},\var{y},\var{z} & \geq 0 \\
   \var{x}-2\var{y}+2\var{z} & \leq -2 + \varepsilon
   \end{align*}

for different positive values of :math:`\varepsilon` would produce
several compromise solutions.

In the example on assignment, considering the extrem solution with
assignment values 8 and 25, we might optimize the total training time,
imposing that the assignment index is, say, at most 9. In this case an
assignment with values 9, 22 is obtained. Increasing the right hand
side for the first objective to 12, a solution with objectives 12,19
is obtained, which seems to be quite a good compromise. This way a set
of interesting solutions can be automatically built and proposed to
the decision maker.

Another possibility, which becomes particularly reasonable when the
different objective functions share the same unit of measurement and,
possibly, the same range of values, is to build a surrogate objective
function by a linear combination:

.. math::
   :nowrap:

   \begin{align*}
   \min & \sum_{j} \lambda_{j} f_{j} (\var{x}) \\
   \var{x} & \in X;
   \end{align*}

where the non negative coefficients :math:`\lambda_j`  can be chosen
to reflect the different importance of the objectives and to span the
set of feasible solution to generate several candidates.

Finally, another possibility is to use a :index:`goal programming`
approach, consisting in choosing non negative constants
:math:`\lambda_j`, exactly as in the previous approach, and solving

.. math::
   :nowrap:

   \begin{align*}
   \min_{\var{x},\var{y}} \sum_{j} \lambda_{j} \var{y}_{j} & \\
   f_{j} (\var{x}) & \leq \var{y}_{j} & \forall \, j \\
   \var{x} & \in X \\
   \var{y}_j & \geq \param{\bar{y}}_j
   \end{align*}

where :math:`\param{\bar{y}}_j` is the minimum value of objective
:math:`j` when optimized separately from all the others. In this
approach we seek for a compromise solution by "pushing" the right
hand sides as much as possible towards low values. It can be proven
that this approach is in a sense equivalent to the previous one.

****************************************************************
Some nonlinear problems that can be transformed into linear ones
****************************************************************

There exist a tiny set of non linear optimization problems which,
thanks to sometimes rather ingenious tricks,
can be transformed into equivalent linear ones. These few
cases are extremely important, as they correspond to a large and very
interesting set of possible applications.

.. _sect:minmax:

================
Minimax problems
================

Consider the optimization  problem

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{x} \in X} \max \{f_1 (\var{x}), f_2 (\var{x}), \ldots, f_k (\var{x}) \}
   \end{align*}
 
This is  a nonlinear problem and, frequently,  a non smooth one, by
which we mean that it is not granted that the overall objective
function is everywhere continuously differentiable, even if each of
the single functions is. Even in the linear case, differentiability is
in general lost, and the function is
piecewise linear.

Minimax models can  be seen  as another way to tackle 
multiobjective optimization problems: in order  to
simultaneously  minimize multiple objectives, here we minimize the
pointwise  worst one. If we
associate to each feasible solution  a point in the
space of objective function values whose coordinates are the values ​​of individuals
objectives, the minimax approach corresponds to finding that
feasible solution  which corresponds   to the minimal :math:`\ell^\infty` norm.

Consider as a simple illustrative problem the following one:

.. math::
   :nowrap:

   \begin{align*}
   \min_{\var{x},\var{y}} &\max \{\var{x}-\var{y}; -\var{x}+
   2\var{y}\}\\
   \var{x} + \var{y} & \leq 3 \\
   \var{y} & \leq 2 \\
   \var{x}, \var{y} & \geq 0
   \end{align*}

.. tikz::

   \begin{tikzpicture}
   \draw[help lines] (0,0) grid (4,3);
   \draw[thick,->] (-0.2,0) -- (4.2,0) node[right] {$x$};
   \draw[thick,->] (0,-0.2) -- (0,3.2) node[above] {$y$};
   \coordinate [label=below left:O] (o) at (0,0);
   \coordinate [label=above left:A] (a) at (0,2);
   \coordinate [label=above right:B] (b) at (1,2);
   \coordinate [label=above right:C] (c) at (3,0);
   \draw[-, fill=orange!30, opacity=.5] (o)--(a)--(b)--(c)--cycle;
   \draw[very thin,gray] (0,0) -- (1.8,1.2);
   \draw[thick,dotted,blue] (1.5,1) -- (1.75,1.25);
   \draw[thick,dotted,blue] (0,0) -- (1.5,1.5);
   \draw[thick,dotted,blue] (0,0.5) -- (1.25,1.75);
   \draw[thick,dotted,blue] (0,1) -- (1,2);
   \draw[thick,dotted,blue] (0,1.5) -- (.5,2);
   \draw[->,blue,thick] (0,0) -- (0.7071,-0.7071);
   \draw[thick,dotted,cyan] (0,0) -- (2,1);
   \draw[thick,dotted,cyan] (0.5,0) -- (2.1666,.83333);
   \draw[thick,dotted,cyan] (1,0) -- (2.3333,.6666);
   \draw[thick,dotted,cyan] (1.5,0) -- (2.5,.5);
   \draw[thick,dotted,cyan] (2,0) -- (2.66666,.3333);
   \draw[->,cyan,thick] (0,0) -- (-0.4472,0.8944);
   \end{tikzpicture}

In the figure above we report the feasible set, a few level set of the
objective function and the gradients of the two components of the
objective functions (we recall that the gradient of a function points
towards increasing levels and is normal to the level curve). Along the
gray line, which corresponds to the set on which the two functions in
the objective have the same value, the objective function is non
differentiable.

It is possible to transform the minimax problem into an equivalent
one, according to the following schee.

Let us first introduce an auxiliary variable :math:`z` and
re-write the problem in the following equivalent form:

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{x}, \var{z}} \var{z} &  \\
   \var{z} & = \max \{f_1 (\var{x}), f_2 (\var{x}), \ldots, f_k (\var{x}) \} \\
   \var{x} & \in X 
   \end{align*}

where the nonlinearity of the objective function has been simply
transferred into a  constraint. Recalling the definition of the
maximum of  a finite set of numbers we can relax the above problem as:

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{x}, \var{z}} \var{z}  \\
   \var{z} & \geq f_{i} (\var{x})  & \forall \, i = 1, k  \\
   \var{x} & \in X 
   \end{align*}

This is indeed a relaxation as the requirement :math:`\var{z}  \geq
f_{i} (\var{x})`  is necessary but not sufficient for :math:`\var{z}`
to be the maximum of the :math:`k` functions: it is in fact lacking a
required constraint which requires :math:`\var{z}` to be exactly equal to
at least one of the :math:`k` functions.
In other words, the constraints correctly model only the requirement

.. math::
   :nowrap:
      
   \[
   \var{z} \geq \max_{i=1,k} f_{i} (\var{x})
   \]
   
Therefore, a priori, feasible solutions might exist in which the
value of :math:`\var{z}` is strictly greater than the maximum
of the objectives.

However it is easy to show that the model is indeed correct when the
optimal solution is considered. In fact,
assume that an optimal solution has
:math:`(\var{x}^\star,\var{z}^\star)` has been found and that

.. math::
   :nowrap:

   \[
   \var{z}^\star > f_i(\var{x}^\star) \qquad \forall\, i
   \]

In this case, 
since no other constraint is imposed on :math:`\var{z}`
it would be possible to find another solution
:math:`(\var{x}^\star,\var{z}^\star-\varepsilon)` with
:math:`\varepsilon >0` which is still feasible but with a lower
value of the objective function. But this is absurd, as it was assumed
that the solution was indeed optimal.

When both the objective functions and the constraints are linear, this
transformation reduces the problem to a linear optimization one.
Following a very similar set of transformation, it is very easy to see
how maximin problems can be transformed into equivalent ones.

It is however not possible to use the same technique to transform
*minimin*  or  *maximax* problems, as the objective function will not
anymore "push" the auxiliary variable against one of the objective
functions. Apparently this case would require the use of disjunctive
(logical OR) constraints to model the fact that the auxiliary variable
needs to be equal to the first, or to the second, or ..., or to the
last function. However, in these special cases, it is quite easy to
solve the problem anyway. In fact, e.g., for a minimin problem, we can
minimize each single function and return the optimal solution
associated with the minimum of the minima found. Thus the problem can
be solved by finding the optimal solution to :math:`k` problems

.. math::
   :nowrap:

   \begin{align*}
   z_i^\star & = \min_{\var{x} \in X} f_i(\var{x})
   \end{align*}

and returning :math:`\min_i z_i^\star` as the optimal value.

Minimax problems (and the others discussed here) have many
applications. As an example, most taxation systems define the taxes to
be paid as a piecewise linear function with increasing slopes: this
rule follows the general principle that not only those who have more
should pay more taxes, but the percentage of taxes paid should be
greater than that of lower income persons. The current (as of 2021)
taxation rate for Italian income is depicted in the following graph: 

.. tikz::

   \begin{tikzpicture}[yscale=0.25]
   \draw[->] (0,0) -- coordinate (x axis mid) (10,0);
   \draw[->] (0,0) -- coordinate (y axis mid)(0,40);
   \foreach \x in {1,2,3,4,5,6,7,8,9}
   \draw  (\x,.1) -- (\x,-.3)
   node[anchor=north] {$\x0$};
   \foreach \y/\lab in {5,10,15,20,25,30,35,40}
   \draw (.1,\y) -- (-.3,\y) node[anchor=east] {$\y$};
   \node[below=1] at (x axis mid) {Income};
   \node[left=2,rotate=90] at (y axis mid) {Taxes};
   \draw[thick] (0,0) -- (1.5,3.45) -- (2.8,6.96) -- (5.5,17.22) --
   (7.5,25.42) -- (10.0,36.42);
   \foreach \x / \y in {1.5/3.45,2.8/6.96,5.5/17.22,7.5/25.42}
   \draw[dotted] (\x,0) -- (\x,\y);
   \end{tikzpicture}

It can be readily seen that the function corresponding to the  5
increasing  slopes in the above graphical representation might be
obtained as the pointwise maximum of 5 straight lines.

.. _fair_assignment:
   
:application: :index:`Fair assignment`

   The problem of fair assignment has been already introduced when the
   bi-partite assignment problem was introduced. The mathematical
   formulation is the following:
   
   .. math::
      :nowrap:
      
      \begin{align*}
      \min \max_{(i, j) \in \set{Arcs}} \{\param{Cost}_{ij}
      \var{f}_{ij} \} & \\
      \sum_{i: (i,j) \in \set{Arcs}} \var{f}_{ij} & = 1 & \forall\,i \\
      \sum_{j: (i,j) \in \set{Arcs}} \var{f}_{ij} & = 1 & \forall\,j
      \\
      \var{f}_{ij} & \in \{0,1\} & \forall\, (i,j)
      \end{align*}

   Using the transformation technique just presented, the problem can
   be equivalently written as
   
   .. math::
      :nowrap:
	 
      
      \begin{align*}
      \min \var{z} & \\
      \var{z} & \geq \param{Cost}_{ij} \var{f}_{ij}  & \forall\, (i,j)\\
      \sum_{i: (i,j) \in \set{Arcs}} \var{f}_{ij} & = 1 & \forall\,i \\
      \sum_{j: (i,j) \in \set{Arcs}} \var{f}_{ij} & = 1 & \forall\,j
      \\
      \var{f}_{ij} & \in \{0,1\} & \forall\, (i,j)
      \end{align*}

   This formulation, although linear, has also integrality
   requirements on the main variable which cannot be canceled, as the
   problem is no more a network flow one and, indeed, solving the
   continuous relaxation usually produces fractional solutions. It
   might be observed that this problem  can be efficiently solved by
   specialized algorithms; however this linear representation does not
   enjoy the integrality property.

:application: :index:`Zero-sum games`

   An application which is strictly linked with the birth of
   :index:`duality theory` is that to
   :index:`non cooperative zero-sum games` (see, e.g.,
   :ref:`vonneumann44`). The example we will be
   using in this presentation is related to a child game, the "Chinese
   morra", but the resulting model is general. Assume there are two
   players, denoted by R and C (these names are linked to the Rows and
   Columns of a matrix we will be introducing shortly). The two
   players, simultaneously and without any communication, have to
   choose one out of :math:`n=3` possible "moves":

   - r (Rock)
   - p (Paper)
   - s (Scissor)

   and the general rule is that Rock wins over Scissor, but loses with
   Paper, while Paper loses with Scissor. If the moves chosen by the
   two players are different, 
   one of the two players wins; otherwise, if they both choose the
   same move, there is a tie. In a standard game players just count
   the number of wins and losses. It is easy to imagine, although this
   can be formally proven, that in the standard case no player has an
   a priori advantage over the other. Moreover the "best" strategy of
   both players is to choose, uniformly, a move at random. However we
   might think of a variant of the game in which the amount to win or
   lose in a game depend on the chosen moves. As an example, consider
   the following matrix:

   .. math::
      :nowrap:

      \begin{align*}
      Q & =
      \begin{array}{c|rrr}
        & r & p & s \\ \hline
      r & 0 & 1 & -3 \\ 
      p & -2 &0 & 5 \\
      s & 6 & -4 & 0 \\ \hline
      \end{array}
      \end{align*}

   This matrix, called a :index:`payoff matrix` has the following
   interpretation: assume player R chooses row :math:`i` in the
   matrix and player C chooses column :math:`j`.  Then player R will
   "gain" an amount :math:`\param{Q}_{ij}`, while player C will "lose" the same
   amount. When a gain is negative, it is indeed a cost and when a
   loss is negative, it corresponds to a gain. As an example if player
   R chooses Rock and player C chooses Scissor, R will lose 2 and C
   will gain 2.

   A :index:`strategy` for player R is defined as a vector
   :math:`\var{x} \in \R^n` such that

   .. math::
      :nowrap:

      \begin{align*}
      \sum_{i=1}^n \var{x}_i & =  1 \\
      \var{x}_i & \geq 0 & \forall\, i=1,n
      \end{align*}

   We can define in a totally analogous way a strategy for the second
   player. A strategy is thus defined as the choice of a discrete
   probability distribution. Given a strategy, a player will decide a
   move based on a random sampling from the chosen distribution. A
   :index:`pure strategy` or a :index:`deterministic strategy` is
   given by a binary strategy vector. In other words, in a pure
   strategy we choose, with probability one, a specific move, without
   any randomization.

   In order to analyze a possible way to build a rational strategy,
   assume, for the moment, that player C is in the best possible
   condition, i.e., that player C knows the strategy of player R,
   :math:`\bar{x}`. Thus, we can evaluate the loss  of player C,
   depending on the chosen strategy :math:`\var{y}`, as

   .. math::
      :nowrap:

      \begin{align*}
      \var{y}_1 (\sum_i \bar{x}_i \param{Q}_{i1} ) + 
      \var{y}_2 (\sum_i \bar{x}_i \param{Q}_{i2} ) + \cdots +
      \var{y}_n (\sum_i \bar{x}_i \param{Q}_{in} ) \\
      = \bar{x}^T \param{Q} \var{y}
      \end{align*}

   and the best option for player C will be the one which solves the
   following optimization problem:

   .. math::
      :nowrap:

      \begin{align*}
      \max_{\var{y}} \bar{x}^T \param{Q} \var{y} & \\
      \mathbf{1}^T \var{y} & = 1 \\
      \var{y} & \geq 0
      \end{align*}

   This is a standard linear optimization problem whose optimal
   (basic) solution just consists of a single variable in the optimal
   basis. The optimal solution would be, therefore, a pure
   deterministic strategy with value:

   .. math::
      :nowrap:

      \begin{align*}
      \max \{\bar{x}^T \param{Q}_1, \bar{x}^T \param{Q}_w, \ldots, \bar{x}^T \param{Q}_n  \}
      \end{align*}
   
   Here :math:`\param{Q}_j` denotes the :math:`j` -th column of the payoff
   matrix. Going back to the best strategy for player rows, we can now
   assume that this player will try to minimize the expected loss in
   the worst possible case, i.e. when the adversarial discovers the
   strategy of the player. This approach corresponds to a cautious
   strategy which tries to obtain the best possible result in the
   worst possible scenario. It is quite frequently used in games
   against an intelligent opponent. Given this assumption on the
   objective of the R player, we would like to solve the optimization
   problem

   .. math::
      :nowrap:

      \begin{align*}
      \min_{\var{x}} \max \{\var{x}^T \param{Q}_1, \var{x}^T \param{Q}_w, \ldots,
      \var{x}^T \param{Q}_n  \} & \\
      \mathbf{1}^T \var{x} & =1 \\
      \var{x} & \geq 0
      \end{align*}
      
   and this problem is easily recognized to belong to the class of
   minimax problems we analyzed in this chapter. Thus the R player
   best strategy can be found, equivalently, by solving the linear
   optimization problem:
   

   .. math::
      :nowrap:

      \begin{align*}
      \min_{\var{x},\var{z}}  \var{z} \\
      \var{z} & \geq \var{x}^T \param{Q}_j & j=1,n \\
      \mathbf{1}^T \var{x} & = 1\\
      \var{x} & \geq 0
      \end{align*}
      
   A simple implementation of this model follows:

   .. literalinclude:: AMPL/game.mod
      :language: ampl
      :caption: game.mod

   with the data of the current example:

   .. literalinclude:: AMPL/game.dat
      :language: ampl
      :caption: game.dat

   Solving the problem we get the following optimal strategy for
   player R:

   .. math::
      :nowrap:

      \begin{align*}
      \var{x}_p & =  33,6449\% \\
      \var{x}_r & = 54.2056\% \\
      \var{x}_s &= 12.1495\%
      \end{align*}

   with an objective function value :math:`0.05607476636`. This means
   that the game is not balanced and even with the best possible
   strategy (in the minimax sense) the R player will incur in a
   positive expected loss.

   It is quite easy to see that the best (maximin) strategy for the
   opponent, the C player, can be found by following an identical
   path. And, if this is done, we can discover that the best strategy
   of the C player is the solution of the dual problem of the linear
   optimization model faced by the R player. This important result is
   linked to the birth of duality theory and can be traced back to
   discussions between the "father" of linear optimization,
   George B. Dantzig, and the "father" of many scientific advances,
   including the modern computer and the modern theory of economic
   behaviour, John von Neumann.

   Thanks to this property, the best strategy of the opponent can be
   easily and immediately found to be
   
   .. math::
      :nowrap:

      \begin{align*}
      \var{y}_p & = 50.4673 \% \\
      \var{y}_r & = 34.5794\% \\
      \var{y}_s &= 14.9533\%
      \end{align*}

   The model presented here is extremely elementary and might be
   extended in many ways. As an example, the payoff matrix needs not
   be square, as the available actions of the players might differ.
   There might even exist two different payoff matrices, in the so
   called non-zero sum games in which the loss of a player is not
   necessarily equal to the gain of the opponent. There might be more
   than two players, and there might be different ways to define an
   "optimal" strategy. Game theory is extremely rich and deep, and it
   deals with profound concepts like that of equilibrium (Nash
   equilibria as an example) or cooperative and coalition games. It is
   however noticeable how everything started from "elementary"
   duality in linear optimization models.
   
   
================================
Absolute values in the objective
================================

Consider the optimization problem

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{x}, \var{y}} \sum_{j} c_{j} | \var{x}_{j} | + \sum_k d_k \var{y}_k\\
   (\var{x}, \var{y}) & \in  X \\
   \var{y} & \geq  0
   \end{align*}
   
Here some of the variables, which of course are not  constrained to be
non negative, appear
in the objective in absolute value. Similarly to the previous case of
minimax problems, here too the objective function is only piecewise
linear, and in general not everywhere differentiable.
In some cases however the problem can be transformed into an
equivalent one in which the non linear objective is transformed into a
linear one with linear additional constraints.

In order to transform the problem it is worth observing that 
the function :math:`| \var{x} |` can be seen as the
pointwise maximum between two linear functions:

.. math::
   :nowrap:
      
   \begin{align*}
   | \var{x} | = \max \{\var{x}, - \var{x} \}.
   \end{align*}
 
Thus, following the same path as in the transformation of minimax
problems,  the general problem can be represented as

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{x}, \var{y}} \sum_{j} c_{j} \max \{\var{x}_j, - \var{x}_j \} +
   \sum_k d_k \var{y}_k & \\
   (\var{x}, \var{y}) & \in X \\
   \var{y} & \geq 0
   \end{align*}
   
and, introducing  an auxiliary variable :math:`\var{z}_j` for each
:math:`\var{x}_j` variable the problem can be rewritten as

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{x}, \var{y}, \var{z}} \sum_{j} c_{j} \var{z}_j + \sum_k d_k
   \var{y}_k & \\
   \var{z}_j & = \max \{\var{x}_j, - \var{x}_j \} \\
   (\var{x}, \var{y}) & \in X \\
   \var{y}, \var{z} & \geq 0.
   \end{align*}
 
Similarly to the minimax case,  the
constraint associated to the  definition of the auxiliary variables
can be relaxed, but only if the cost coefficients :math:`c_{j}` are
strictly positive:

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{x}, \var{y}, \var{z}} \sum_{j} c_{j} \var{z}_j + \sum_k d_k
   \var{y}_k & \\
   \var{z}_j & \geq \var{x}_j \\
   \var{z}_j & \geq - \var{x}_j \\
   (\var{x}, \var{y}) & \in X \\
   \var{y}, \var{z} & \geq 0
   \end{align*}
 
or

.. math::
   :nowrap:

   \begin{align*}
   \min_{\var{x}, \var{y}, \var{z}} \sum_{j} c_{j} \var{z}_j + \sum_k d_k
   \var{y}_k & \\
   - \var{z}_j & \leq \var{x}_j \leq \var{z}_j \\
   (\var{x}, \var{y}) & \in X \\
   \var{y}, \var{z} & \geq 0.
   \end{align*}

Equivalence, for the case of positive cost coefficients, can be proven
in exactly the same way as we did for minimax problems. If even a
single coefficient  has a negative sign than the whole model fails to
correctly represent the original one and, in general, the problem
becomes computationally much more difficult. We might observe that
with positive coefficients the objective function is *convex*, while
if even a single one is negative convexity is lost. Null coefficients
of course cause no difficulties as the corresponding variable can be
canceled from the objective function.

The same problem can be transformed, under the same assumptions, into
a linear optimization one following a different, yet equivalent, path. In generic linear
optimization models the standard transformation applied to free
variables, i.e., to variables which are not constrained to be non
negative, is to susbsititute those variables with the difference of
two non negative ones:

.. math::
   :nowrap:
      
   \begin{align*}
   \var{x}_{j}  & = \var{x}^+_{j} - \var{x}^-_{j} \\
   \var{x}^+_{j}, \var{x}^-_{j} \& geq 0
   \end{align*}

The optimization problem can thus be written equivalently as

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{x}^+, \var{x}^-, \var{y}} \sum_{j} c_{j} | \var{x}^+_{j} -
   \var{x}^-_j | + \sum_k d_k \var{y}_k  \\ 
   (\var{x}^+ - \var{x}^-, \var{y}) & \in  X \\
   \var{x}^+, \var{x}^+, \var{y} & \geq  0.
   \end{align*}

This substitution transforms the original problem into an equivalent
one; however the mapping between solutions in the two feasible sets is
not unique: if :math:`\var{x}_{j} \geq0` it is always possible to choose

.. math::
   :nowrap:

   \begin{align*}
   \var{x}^+_{j} & = \var{x}_{j} + \mathvar{\Delta}_j \\
   & = | \var{x}_j | + \mathvar{\Delta}_j \\
   \var{x}^-_{j} & = \mathvar{\Delta}_j
   \end{align*}
   
for any choice of :math:`\mathvar{\Delta}_j \geq0`. If on the
contrary, :math:`\var{x}_{j} \leq0`, 
it is possible to choose

.. math::
   :nowrap:
      
   \begin{align*}
   \var{x}^+_{j} & = \mathvar{\Delta}_j \\
   \var{x}^-_{j} & = - \var{x}_{j} + \mathvar{\Delta}_j \\
   & = | \var{x}_j | + \mathvar{\Delta}_j
   \end{align*}
   
with :math:`\mathvar{\Delta}_j \geq0`.

It can be seen that, if :math:`\mathvar{\Delta}_j = 0`, at least one of the two
components, :math:`\mathvar{u}_{j}` or :math:`\mathvar{v}_{j}`, is 0
while the other  is equal to :math:`|x_{j}|`.
It can also be observed thus 
that the *sum* of the two variables is

.. math::
   :nowrap:

   \begin{align*}
   \var{x}^+_j + \var{x}^-_j & =  | \var{x}_j | + 2 \mathvar{\Delta}_j
   \end{align*}
 
Replacing in the objective  function each occurrence of 
:math:`| \var{x}_{j} |` with the *sum* of the components :math:`\var{x}^+_{j}` and
:math:`\var{x}^+_{j}`, if, as it was assumed, the cost
coefficients are positive, in the optimal solution we will surely have
:math:`\mathvar{\Delta} = 0`).
 
The model is therefore equivalent to the following:

.. math::
   :nowrap:
      
      \begin{align*}
      \min_{\var{x}^+, \var{x}^-, \var{y}} \sum_{j} c_{j} (\var{x}^+_{j} + \var{x}^-_j) + \sum_k d_k \var{y}_k &  \\
      (\var{x}^+ - \var{x}^-, \var{y}) & \in  X \\
      \var{x}^+, \var{x}^-, \var{y} & \geq  0.
      \end{align*}
 
It is important to recall  that if even a single  coefficient :math:`c_{j}`
was negative, the linear model would not be correct: the
negative cost factor would in this case be an incentive a
choose both :math:`\var{x}^+_{j}` and  :math:`\var{x}^-_{j}`  not null
and as high as possible (leading, in fact, to an unbounded problem).
 
It is important to observe that the replacement of a free variable
with the *sum* of the two auxiliary ones is done 
only in the objective, while in all other parts of the model the 
usual transformation is applied. as an  example, the problem

.. math::
   :nowrap:
      
   \begin{align*}
   \min | \var{x} | + 2 | \var{y} | - \var{z} & \\
   \var{x} - \var{y} + 2\var{z} & \geq 0 \\
   -\var{x} + 2\var{y} + \var{z} & \leq 1 \\
   \var{z} & \geq 0
   \end{align*}
   
can be transformed into the equivalent linear problem

.. math::
   :nowrap:
      
   \begin{align*}
   \min \var{x}^+ + \var{x}^- + 2\var{y}^+ + 2\var{y}^ - - \var{z} & \\
   \var{x}^+ - \var{x}^- - \var{y}^+ + \var{y}^- + 2\var{z} & \geq 0 \\
   -\var{x}^+ + \var{x}^- + 2\var{y}^+ -2 \var{y}^- + \var{z} & \leq 1 \\
   \var{x}^+, \var{x}^-, \var{y}^+, \var{y}^-, \var{z} & \geq 0.
   \end{align*}
 

=========================================
Regression (or best approximation) models
=========================================

One of the most important and
frequently used  applications of  numerical optimization
consists of the approximation of  experimental data with analytical
models. The idea of best approximating data with a function from a
sufficiently rich family of models is quite old and is motivated both
by the desire to understand relationships among data and by the need
of producing reliable forecasts. 

As an example, consider the data represented
in the following picture:

.. tikz::

   \begin{tikzpicture}[xscale=0.2]
   \draw[help lines] (0,0) grid[xstep=10]  (50,5);
   \draw[->, thick] (-0.5,0) -- node[below=0.5] {body mass} (51,0);
   \draw[->, thick] (0,-0.5) -- node[left=0.8,rotate=90] {brain mass}(0,5.5);
   \foreach \x in {5,10,15,20,25,30,35,40,45} {
   \draw (\x,-0.1) node[below]{\small{\x}} -- (\x,0.1);
   };
   \foreach \y in {1,2,3,4,5}
   {\draw (-0.5,\y) node[above,rotate=90] {\small{\y}} -- (0.5,\y);};
   \foreach \x / \y in {
   0.14/0.005,
   0.25/0.010,
   0.30/0.023,
   0.33/0.048,
   0.40/0.023,
   1.00/0.060,
   1.00/0.120,
   1.20/0.075,
   1.90/0.280,
   2.40/0.550,
   2.50/0.104,
   2.60/0.900,
   3.00/0.122,
   3.50/0.785,
   3.90/3.500,
   4.00/0.101,
   5.00/0.200,
   5.50/1.040,
   5.70/0.920,
   6.30/1.700,
   6.40/0.425,
   6.60/1.000,
   8.10/1.350,
   10.80/3.500,
   11.40/1.620,
   12.10/2.500,
   12.30/0.750,
   12.30/2.000,
   12.50/1.400,
   15.50/0.480,
   17.00/4.050,
   17.50/1.410,
   21.00/3.600,
   25.00/3.000,
   25.60/3.300,
   39.20/4.288,
   44.50/3.385,
   50.40/4.235   } \draw[blue] (\x,\y) ellipse(3pt and 0.5pt);
   %\draw[red] (0,1.58275) -- (50,50*0.0419847+1.58275); 
   %\draw[green] (0,0.0479401) -- (50,50*0.108165+0.0479401); 
   %\draw[brown] (0,0.428914) -- (50,50*0.0916022+0.428914);
   \end{tikzpicture}

The picture reports  a few experimental observations of the mass
of the brain versus the  body mass in some mammal species.
From this picture it is reasonable to assume a  correlation between the
two masses. We would like to fit a line to the data so as to
"minimize" a measure of the error.  There are many assumptions to be
done in order to frame this problem in a correct way. Let us consider
the following picture representing a possible fit to the above data
through a linear model :math:`y = ax + b`:

.. tikz::

   \begin{tikzpicture}[xscale=0.2]
   \draw[help lines] (0,0) grid[xstep=10]  (50,5);
   \draw[->, thick] (-0.5,0) -- node[below=0.5] {body mass} (51,0);
   \draw[->, thick] (0,-0.5) -- node[left=0.8,rotate=90] {brain mass}(0,5.5);
   \foreach \x in {5,10,15,20,25,30,35,40,45} {
   \draw (\x,-0.1) node[below]{\small{\x}} -- (\x,0.1);
   };
   \foreach \y in {1,2,3,4,5}
   {\draw (-0.5,\y) node[above,rotate=90] {\small{\y}} -- (0.5,\y);};
   \foreach \x / \y in {
   0.14/0.005,
   0.25/0.010,
   0.30/0.023,
   0.33/0.048,
   0.40/0.023,
   1.00/0.060,
   1.00/0.120,
   1.20/0.075,
   1.90/0.280,
   2.40/0.550,
   2.50/0.104,
   2.60/0.900,
   3.00/0.122,
   3.50/0.785,
   3.90/3.500,
   4.00/0.101,
   5.00/0.200,
   5.50/1.040,
   5.70/0.920,
   6.30/1.700,
   6.40/0.425,
   6.60/1.000,
   8.10/1.350,
   10.80/3.500,
   11.40/1.620,
   12.10/2.500,
   12.30/0.750,
   12.30/2.000,
   12.50/1.400,
   15.50/0.480,
   17.00/4.050,
   17.50/1.410,
   21.00/3.600,
   25.00/3.000,
   25.60/3.300,
   39.20/4.288,
   44.50/3.385,
   50.40/4.235   } \draw[blue] (\x,\y) ellipse(3pt and 0.5pt);
   \draw[cyan] (17,4.05) ellipse(4.5pt and 0.75pt);
   \node[above left] at (17,4.05) {\small{$P$}};
   \node[below] at (17,1.7) {\small{$P_1$}};
   \node[below] at (40,4.05) {\small{$P_2$}};
   \node[below] at (22,2.2) {\small{$P_3$}};
   \draw[blue,thick] (0,0) -- (50,5);
   \draw[red,dashed] (17.00,4.050) -- (17,1.7);
   \draw[orange,thick] (17.00,4.050) -- (40,4.05);
   \draw[pink,dotted,thick] (17.00,4.050) -- (22,2.2);
   %\draw[red] (0,1.58275) -- (50,50*0.0419847+1.58275); 
   %\draw[green] (0,0.0479401) -- (50,50*0.108165+0.0479401); 
   %\draw[brown] (0,0.428914) -- (50,50*0.0916022+0.428914);
   \end{tikzpicture}

Is the continuous straight line a good model of the observed data? Can
we trust it as a plausible explanation of the correlation between
brain and body mass? Can we reliably use this line as a predictor for
the brain mass of a yet unseen mammal?
The answer to these question is not easy in general.
The first important problem to be solved is how to measure the error between an
experimental data and an approximation function; this is fundamental
as this definition will guide towards different possible "best"
approximation models.  Assume the simplest possible model, a linear function:

.. math::
   :nowrap:
   
   \begin{align*}
   \param{Y} = \var{a} \param{X} + \var{b}
   \end{align*}
   
where :math:`\var{a}` and :math:`\var{b}` are unknown
variables to be determined.
Consider a single data point, identified by :math:`P` in the above
figure. The most frequently used error measure for this data point is
the (absolute) difference between the ordinate of the point and the
ordinate of the corresponding point in the approximation line. For 
linear approximation, if the coordinates of this data point are
:math:`(\param{X}_i,\param{Y}_i)`, the error is defined as

.. math::
   :nowrap:
      
   \begin{align*}
   \var{E}_i = \param{Y}_i - (\var{a} \param{X}_i + \var{b})
   \end{align*}

In the figure, the absolute value of the error corresponds to the
length of the line segment 
:math:`(P,P_1)`. 
This error measure is related to the assumption, which is reasonable
in many, although not all, cases,
that the observations of the independent variable
:math:`\param{X}` are not affected by measurement errors and
therefore, given an observation  the uncertainty is
associated just  with the ordinate value associated with :math:`\param{X}`.
Of course, this assumption is not always justified:  in our example, for every
data point, both observations (body mass as well as
brain mass) are affected by experimental error and, therefore,  there
seems to be no reasons why  the error with respect to one of the two variables is
to be considered differently from the other one.
In cases like this one, e.g., a more
reasonable model might be based on a definition of the errore as the
(Euclidean )distance between the observation and the regression line.
This distance can be easily computed and, in the one-dimensional
case, it turns out to be equal to:

.. math::
   :nowrap:

   \begin{align*}
   \var{E}_i = \frac{\param{Y}_i - (\var{a} \param{X}_i + \var{b})}{\sqrt{1+a^2}}
   \end{align*}

(again we are defining the error measure with sign, but usually this
value will enter an objective function to be minimized through its
absolute value).
In the figure above this distance is represented by the length  of
segment :math:`(P,P_3)`
Another model, although much less frequently used, measures the error
along the horizontal axis:

.. math::
   :nowrap:

   \begin{align*}
   \var{E}_i = \param{X}_i - (\param{Y}_i - \var{b})/\var{a}
   \end{align*}

as represented in the figure by the length of segment
:math:`(P,P_3)`. In what follows we will use the first of the three
error measures  reported here as it is the most common one in
practice. However our intent here was to draw the reader's attention
to the many diverse possible choices for this fundamental measure. We
observe in passing that also weighted error measures might be
considered, one of them being to measure the *relative* instead of
the *absolute* error, which can be obtained by dividing the classical
error measure by :math:`\param{Y}_i` (obviously, if non zero).

Once an error measure has been defined, the problem remains of
defining an objective function to be minimized. The problem, in fact,
is inherently a multi-objective one, as we would like to minimize all
of the error measures with a single choice of the parameters of the
regression line.
 
Regression models generally assume a  parametric model
like, e.g., a  linear one as  just
introduced. The parameters of the model can then be chosen solving the
problem

.. math::
   :nowrap:
      
   \begin{align*}
   \min F \{| \var{E}_i |, i = 1, \ldots, N \} 
   \end{align*}
   
where :math:`N` is the number of observations made and :math:`F`
is a function designed to aggregate all of the individual error measures into
a single one. As this is indeed a multi-objective problem, some of th
techniques presented before might be employed, like that of minimizing
a linear combination of the absolute errors. We present here a few of
the most interesting approaches for the problem.

^^^^^^^^^^^^^^^^^^
Minimax regression
^^^^^^^^^^^^^^^^^^

If, as an aggregation operator :math:`F` we choose the operator :math:`\max`, in the linear case
we are led to the determination of  the unknown
parameters :math:`\var{a}, \var{b}` by solving

.. math::
   :nowrap:
      
   \[
   \min_{\var{a}, \var{b}} \max_{i = 1, \ldots, N} | \param{Y}_{i} - \var{a} \param{X}_{ i} - \var{b} |
   \]
   
This can be easily transformed into a linear optimization  problem. In fact, using the
transformation just introduced  for minimax problems
introducing an auxiliary variable we can write

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{a}, \var{b}, \var{z}} \var{z} & \\
   | \param{Y}_{i} - \var{a} \param{X}_{i} - \var{b} | & \leq \var{z}
   &  \forall \, i = 1, \ldots, N
   \end{align*}
   
and, thanks to the well known properties of the absolute value, the
problem  can be  transformed  in the equivalent linear optimization
form

.. math::
   :nowrap:
      
   \begin{align*}
   \min_{\var{a}, \var{b}, \var{z}} \var{z} & \\
   \param{Y}_{i} - \var{a} \param{X}_{i} - \var{b} & \geq - \var{z}  & \forall \, i = 1, \ldots, N \\
   \param{Y}_{i} - \var{a} \param{X}_{i} - \var{b}  & \leq  \var{z}  & \forall \, i = 1, \ldots, N
   \end{align*}
 
The case just presented is based on linear approximation of
bi-variate data; that is, each observation corresponds to a
pair of numerical values, one of which is typically associated with
an independent variable, or under the control of the
experimenter, while the other one is
the actual observation of the phenomenon, and corresponds to the
dependent variable.

Nothing prevents us from considering
more general models. 
As an example, it is  possible that
observations are associated with more than one independent variable. In
this case we  the term  *multiple regression* is used. Assuming again
a linear model,  we  would like to
estimate  the parameters of the function

.. math::
   :nowrap:
      
   \[
   \sum_{j=1}^k \var{a}_j x_j + \var{b}
   \]

so that  an aggregate measure of the error is minimized. For  minimax regression, 
the following general model can be defined:

    
.. role:: model
   :class: smallcaps

	   
:model: Multivariate Minimax Linear Regression

|
   :model:`Sets:`
|

   *     :math:`\set{L}`: a set of labels associated with data;

|
   :model:`Parameters:`
|

   * :math:`\param{K}`: the dimension of the "independent variables" (equal
     to one in the univariate case)
   * :math:`\param{Xdata}_{ij}`: value of component :math:`j \in \{1,K\}`
     of the  :math:`i` -th observation, :math:`i \in \set{L}` ;
     in the case of simple univariate regression, 
     this is just the abscissa of  observation :math:`i`. As an 
     example, assume we would like to approximate 
     the sale price of a security, for example a treasury bill,
     and other variates, like, e.g.,  the exchange rate of  euro against  dollar
     and the consumer price index (inflation rate); for each time instant :math:`i`
     there will be two observations of "independent" variables, one
     relating to the Euro / dollar rate, 
     the other to the consumer price index
   * :math:`\param{Ydata}_i`: observed value of the dependent
     variable, :math:`i \in \set{L}`; 
     in the example above, this is the observed value of the 
     treasury bill sale price; 

|
   :model:`Variables:`
|

   *  :math:`\var{a}_j`: value of   parameter :math:`j \in
      \{1,\param{K}\}` in the linear regression model;
   *  :math:`\var{b}`: constant term in the linear regression formula
   *  :math:`\var{z}`: auxiliary variable, necessary for the
      transformation of the minimax problem into an equivalent linear one; 

|
   :model:`Constraints:`
|

   *  bounds of the maximum error in the regression:

      .. math::
	 :nowrap:

	 \[
         - \var{z} \leq \param{Ydata}_i -
         \sum_{j \in \{1,\param{K}\}} \left (\param{Xdata}_{ij} \var{a}_j + \var{b} \right)
         \leq \var{z}
	 \]
	 
    for every :math:`i \in E`;
   
|
   :model:`Objective:` 
|

   * minimization of the maximum error:

     .. math::
	:nowrap:
	   
	\[
	\min \var{z}
	\]
 

An example of implementation of this model for the univariate case is
given below:


.. tabs::
   
   .. group-tab:: AMPL
      .. code-block:: ampl
      .. literalinclude:: AMPL/r-minmax.mod
	 :caption: r-minmax.mod
      
   .. group-tab:: Julia
      .. code-block:: Julia
      .. literalinclude:: Julia/TBD.jl
	 :caption: r-minmax.jl

and, running this model on the brain mass vs body mass example  the
following result is obtained, corresponding to
:math:`\var{a}=0.0916022, \var{b}=0.428914`:

.. tikz::

   \begin{tikzpicture}[xscale=0.2]
   \draw[help lines] (0,0) grid[xstep=10]  (50,5);
   \draw[->, thick] (-0.5,0) -- node[below=0.5] {body mass} (51,0);
   \draw[->, thick] (0,-0.5) -- node[left=0.8,rotate=90] {brain mass}(0,5.5);
   \foreach \x in {5,10,15,20,25,30,35,40,45} {
   \draw (\x,-0.1) node[below]{\small{\x}} -- (\x,0.1);
   };
   \foreach \y in {1,2,3,4,5}
   {\draw (-0.5,\y) node[above,rotate=90] {\small{\y}} -- (0.5,\y);};
   \foreach \x / \y in {
   0.14/0.005,
   0.25/0.010,
   0.30/0.023,
   0.33/0.048,
   0.40/0.023,
   1.00/0.060,
   1.00/0.120,
   1.20/0.075,
   1.90/0.280,
   2.40/0.550,
   2.50/0.104,
   2.60/0.900,
   3.00/0.122,
   3.50/0.785,
   3.90/3.500,
   4.00/0.101,
   5.00/0.200,
   5.50/1.040,
   5.70/0.920,
   6.30/1.700,
   6.40/0.425,
   6.60/1.000,
   8.10/1.350,
   10.80/3.500,
   11.40/1.620,
   12.10/2.500,
   12.30/0.750,
   12.30/2.000,
   12.50/1.400,
   15.50/0.480,
   17.00/4.050,
   17.50/1.410,
   21.00/3.600,
   25.00/3.000,
   25.60/3.300,
   39.20/4.288,
   44.50/3.385,
   50.40/4.235   } \draw[blue] (\x,\y) ellipse(3pt and 0.5pt);
   \draw[red] (0,1.58275) -- (50,50*0.0419847+1.58275); 
   %\draw[green] (0,0.0479401) -- (50,50*0.108165+0.0479401); 
   %\draw[brown] (0,0.428914) -- (50,50*0.0916022+0.428914);
   \end{tikzpicture}

 
Visual inspection reveals that this  approximation is of quite bad
quality, with significant errors in most data points.
 
Minimax  (or *infinite*) regression models
minimize  the :math:`\ell^{\infty}`  norm of the error vector and, thus,
are very  sensitive to "bad" observations or to "outliers". However,
these models have the property of spreading errors quite evenly among
the data points, which, in some application, might be a desired
feature.

^^^^^^^^^^^^^^^^^^^
Absolute regression
^^^^^^^^^^^^^^^^^^^
In absolute, or :math:`\ell^{1}` regression models the
the aggregation operator :math:`F` is defined as  the sum of
absolute differences between model prediction and observed data:

.. math::
   :nowrap:

   \[
   F = \sum_i |\var{E}_i| = \sum_{i = 1}^{N} | \param{Y}_{i} -
   \var{a} \param{X}_{i} - \var{b} | 
   \]
   
 
An absolute regression model can be formulated as follows, taking into
account that sets and parameters are exactly the same as in the
minimax regression case:


	   
:model: Multivariate Absolute Linear Regression


|
   :model:`Variables:`
|

   *  :math:`\var{a}_j`: value of   parameter :math:`j \in
      \{1,\param{K}\}` in the linear regression model;
   *  :math:`\var{b}`: constant term in the linear regression formula
   *  :math:`\var{z}_i`: auxiliary variables, :math:`i \in \set{L}`; 

|
   :model:`Constraints:`
|

   *  bounds of the maximum error in the regression:

      .. math::
	 :nowrap:

	 \[
         - \var{z}_i \leq \param{Ydata}_i -
         \sum_{j \in \{1,\param{K}\}} \left (\param{Xdata}_{ij} \var{a}_j + \var{b} \right)
         \leq \var{z}_i
	 \]
	 
    for every :math:`i \in E`;
   
|
   :model:`Objective:` 
|

   * minimization of sum of absolute errors:

     .. math::
	:nowrap:
	   
	\[
	\min \sum_{i \in \set{L}}\var{z}_i
	\]
 
In the following a possible implementation is reported:


.. tabs::
   
   .. group-tab:: AMPL
      .. code-block:: ampl
      .. literalinclude:: AMPL/r-absolute.mod
	 :caption: r-absolute.mod
      
   .. group-tab:: Julia
      .. code-block:: Julia
      .. literalinclude:: Julia/TBD.jl
	 :caption: r-absolute.jl

and, running this model on the brain mass vs body mass example  the
following result is obtained, corresponding to
:math:`\var{a}=0.108165, \var{b}=0.0479401`:

.. tikz::

   \begin{tikzpicture}[xscale=0.2]
   \draw[help lines] (0,0) grid[xstep=10]  (50,5);
   \draw[->, thick] (-0.5,0) -- node[below=0.5] {body mass} (51,0);
   \draw[->, thick] (0,-0.5) -- node[left=0.8,rotate=90] {brain mass}(0,5.5);
   \foreach \x in {5,10,15,20,25,30,35,40,45} {
   \draw (\x,-0.1) node[below]{\small{\x}} -- (\x,0.1);
   };
   \foreach \y in {1,2,3,4,5}
   {\draw (-0.5,\y) node[above,rotate=90] {\small{\y}} -- (0.5,\y);};
   \foreach \x / \y in {
   0.14/0.005,
   0.25/0.010,
   0.30/0.023,
   0.33/0.048,
   0.40/0.023,
   1.00/0.060,
   1.00/0.120,
   1.20/0.075,
   1.90/0.280,
   2.40/0.550,
   2.50/0.104,
   2.60/0.900,
   3.00/0.122,
   3.50/0.785,
   3.90/3.500,
   4.00/0.101,
   5.00/0.200,
   5.50/1.040,
   5.70/0.920,
   6.30/1.700,
   6.40/0.425,
   6.60/1.000,
   8.10/1.350,
   10.80/3.500,
   11.40/1.620,
   12.10/2.500,
   12.30/0.750,
   12.30/2.000,
   12.50/1.400,
   15.50/0.480,
   17.00/4.050,
   17.50/1.410,
   21.00/3.600,
   25.00/3.000,
   25.60/3.300,
   39.20/4.288,
   44.50/3.385,
   50.40/4.235   } \draw[blue] (\x,\y) ellipse(3pt and 0.5pt);
   %\draw[red] (0,1.58275) -- (50,50*0.0419847+1.58275); 
   \draw[green] (0,0.0479401) -- (50,50*0.108165+0.0479401); 
   %\draw[brown] (0,0.428914) -- (50,50*0.0916022+0.428914);
   \end{tikzpicture}


^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Quadratic (least squares) regression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In most cases regression models are estimated by means of least
squares, or :math:`\ell^2`, error minimization. In the quadratic, or
lest squares, model  the aggregation function takes the fomr:

.. math::
   :nowrap:
      
   \[
   F = \sum_i (\var{E_i})^2 = \sum_{i = 1}^{N} (\param{Y}_{i} -
   \var{a} \param{X}_{i} - \var{b})^{2} 
   \]
   
This is a non linear model, but it is reported here for the sake of
comparison with the other regression models presented.
Among the reasons for the popularity of this model
surely there is the fact that the optimal estimate of parameters
:math:`\var{a}` and :math:`\var{b}` in the linear case
can be carried out using well known analytical expressions in
closed, explicit form.
 
It is not reasonable to use a nonlinear optimization algorithm to fit,
e.g., the mass data used in this chapter. However this would be easy
to do and might prove useful for more complex cases in which an
analytical expression of the optimal parameters is not available. This
might happen, e.g., in nonlinear regression, or when constraints are
imposed over some of the parameters.
The regression line obtained through quadratic regression is somewhat
intermediate between the minimax and the absolute one. In fact, the
distortion caused by squaring the errors make the resulting line quite
sensitive to large errors or to outliers, even if not in an extreme
way as in the case of minimax.

The following picture reports the three regression lines for the same
dataset. For quadratic regression the parameters are
:math:`a=0.0916022, b=0.428914`:

.. tikz::

   \begin{tikzpicture}[xscale=0.2]
   \draw[help lines] (0,0) grid[xstep=10]  (50,5);
   \draw[->, thick] (-0.5,0) -- node[below=0.5] {body mass} (51,0);
   \draw[->, thick] (0,-0.5) -- node[left=0.8,rotate=90] {brain mass}(0,5.5);
   \foreach \x in {5,10,15,20,25,30,35,40,45} {
   \draw (\x,-0.1) node[below]{\small{\x}} -- (\x,0.1);
   };
   \foreach \y in {1,2,3,4,5}
   {\draw (-0.5,\y) node[above,rotate=90] {\small{\y}} -- (0.5,\y);};
   \foreach \x / \y in {
   0.14/0.005,
   0.25/0.010,
   0.30/0.023,
   0.33/0.048,
   0.40/0.023,
   1.00/0.060,
   1.00/0.120,
   1.20/0.075,
   1.90/0.280,
   2.40/0.550,
   2.50/0.104,
   2.60/0.900,
   3.00/0.122,
   3.50/0.785,
   3.90/3.500,
   4.00/0.101,
   5.00/0.200,
   5.50/1.040,
   5.70/0.920,
   6.30/1.700,
   6.40/0.425,
   6.60/1.000,
   8.10/1.350,
   10.80/3.500,
   11.40/1.620,
   12.10/2.500,
   12.30/0.750,
   12.30/2.000,
   12.50/1.400,
   15.50/0.480,
   17.00/4.050,
   17.50/1.410,
   21.00/3.600,
   25.00/3.000,
   25.60/3.300,
   39.20/4.288,
   44.50/3.385,
   50.40/4.235   } \draw[blue] (\x,\y) ellipse(3pt and 0.5pt);
   \draw[red] (0,1.58275) -- (50,50*0.0419847+1.58275) node[below] {\tiny{Minimax}}; 
   \draw[green] (0,0.0479401) -- (50,50*0.108165+0.0479401) node[left] {\tiny{Absolute}}; 
   \draw[brown] (0,0.428914) -- (50,50*0.0916022+0.428914) node[left] {\tiny{Quadratic}};
   \end{tikzpicture}

In order to better appreciate the differences between the three
regression models just presented, in the following figures histograms
of the errors are reported.
   
.. tikz::
   
   \begin{tikzpicture}
   \begin{axis}[ybar interval,
   xtick=data,
   xticklabel interval boundaries,
   x tick label style=
   {rotate=90,anchor=east}, title={minimax errors}
   ]
   \addplot coordinates
   {(0,3)
   (0.25,3)
   (0.5,4)
   (0.75,6)
   (1,3)
   (1.25,4)
   (1.5,12)
   (1.75,3)
   (2,0)
   (2.25,0)
   (2.5,0)
   (2.75,0)
   (3,0)  (3.25,0)};
   \end{axis}
   \end{tikzpicture}

.. tikz::
   
   \begin{tikzpicture}
   \begin{axis}[ybar interval,
   xtick=data,
   xticklabel interval boundaries,
   x tick label style=
   {rotate=90,anchor=east}, title={absolute errors}
   ]
   \addplot coordinates
   {(0.,15)
   (0.25,10)
   (0.5,4)
   (0.75,1)
   (1,2)
   (1.25,3)
   (1.5,0)
   (1.75,0)
   (2,1)
   (2.25,1)
   (2.5,0)
   (2.75,0)
   (3,1) (3.25,0)};
   \end{axis}
   \end{tikzpicture}

.. tikz::

   \begin{tikzpicture}
   \begin{axis}[ybar interval,
   xtick=data,
   xticklabel interval boundaries,
   x tick label style=
   {rotate=90,anchor=east}, title={quadratic errors}
   ]
   \addplot coordinates
   {(0.,9)
   (0.25,12)
   (0.5,8)
   (0.75,3)
   (1,2)
   (1.25,1)
   (1.5,0)
   (1.75,0)
   (2,2)
   (2.25,0)
   (2.5,1)
   (2.75,0)
   (3,0) (3.25,0)};
   \end{axis}
   \end{tikzpicture}

From the above histograms we can observe the expected behaviour of
different regression models. Minimax regression is characterized by
many errors, which, however, are never too large. On the contrary,
absolute regression generates a large portion of errors quite close to
zero, but in some rare cases produces very large deviations. Quadratic
regression is somewhat between these two.

As a final remark on the models presented here it is important to
observe that we gave three examples on how to aggregate errors in
order to find a regression line. It should not be too difficult to
extend these models to more general measures, like percentage errors,
or weighted errors, in which some observations need to be estimated
more accurately than others. When the independent variable is time,
another possibility could be that of discounted errors, i.e., of
giving smaller and smaller weight to errors in past observations.

Another aspect which is worth considering is that the ideas presented
here extend quite nicely to generic parametric approximation
models. If a rich family of functions (a "basis") is given, non linear
regression model might be obtained easily through suitable linear
combination of those functions, and the resulting optimization models
would still be linear optimization ones (or quadratic in the case of
least square regression). Let in fact

.. math::
   :nowrap:

   \[
   \{\phi_k(x)\}_{k=1}^{K}
   \]

be a family of functions. As an example:

.. math::
   :nowrap:

   \[
   \phi_k(x) = x^k
   \]

(for polynomial regression). Then a regression model can be obtained
through (linear) optimization by solving

.. math::
   :nowrap:

   \begin{align*}
   \min F(|\var{E}_1|, \ldots, |\var{E}_N|) & \\
   \var{E}_i & = \param{Y}_i - \sum_{k=1}^K \var{\lambda}_k \phi_k(\param{X}_i)
   \end{align*}

More advanced modeling might be employed to obtain *sparse*
regression models (i.e., models with a small number of non zero
:math:`\lambda` coefficients) or low order regression models,
i.e. models for which the largest index of non zero coefficients is as
small as possible. Here the idea is that if, as an example, going from
a quadratic regression model to a cubic one gives only a modest
improvement in the accuracy, then a simpler model is preferred.

Finally it is also important to recall that in many practical cases,
is might be useful to transform the data before 
starting with the  estimation of regression coefficients. These
transformations are aimed towards making, if possible, a linear
dependence between the data more evident.
In the example, using a logarithmic scale for both the
independent variable which for the dependent one becomes evident
a strong linear link, as seen from the figure



.. tikz::

   \begin{tikzpicture}
   \draw[help lines] (-2,-6) grid[xstep=2]  (4,2);
   \draw[->, thick] (-2,-6) -- node[below=.7] {log body mass} (4,-6);
   \draw[->, thick] (-2,-6) -- node[left=0.8,rotate=90] {log brain mass}(-2,2);
   \foreach \x in {-2,-1,0,1,2,3,4} {
   \draw (\x,-6.1) node[below]{\small{\x}} -- (\x,-5.9);
   };
   \foreach \y in {-5,-4,-3,-2,-1,0,1,2}
   {\draw (-2.1,\y) node[above,rotate=90] {\small{\y}} -- (-1.9,\y);};
   \foreach \x / \y in {
   -1.966/-5.298,
   -1.386/-4.605,
   -1.204/-3.772,
   -1.109/-3.037,
   -0.916/-3.772,
   0/-2.813,
   0/-2.12,
   0.182/-2.59,
   0.642/-1.273,
   0.875/-0.598,
   0.916/-2.263,
   0.956/-0.105,
   1.099/-2.104,
   1.253/-0.242,
   1.361/1.253,
   1.386/-2.293,
   1.609/-1.609,
   1.705/0.039,
   1.74/-0.083,
   1.841/0.531,
   1.856/-0.856,
   1.887/0,
   2.092/0.3,
   2.38/1.253,
   2.434/0.482,
   2.493/0.916,
   2.51/-0.288,
   2.51/0.693,
   2.526/0.336,
   2.741/-0.734,
   2.833/1.399,
   2.862/0.344,
   3.045/1.281,
   3.219/1.099,
   3.243/1.194,
   3.669/1.456,
   3.795/1.219,
   3.92/1.443} \fill[blue] (\x,\y) circle(0.1);
   \draw[red] (-2,-3.544) -- (4,1.834) node[below right]
   {minimax};
   \draw[green] (-2,-4.66131) -- (4,2.03985) node[above left]
   {absolute};
   %\draw[red] (0,1.58275) -- (50,50*0.0419847+1.58275); 
   %\draw[green] (0,0.0479401) -- (50,50*0.108165+0.0479401); 
   %\draw[brown] (0,0.428914) -- (50,50*0.0916022+0.428914);
   \end{tikzpicture}

Considering, as an example, absolute regression, the solution in this
case turns out to be :math:`a = 1.11686, b = -2.42759` and, thus,
the model is

.. math::
   :nowrap:

   \begin{align*}
   \log y &  = 1.11686 \log x -2.42759
   \end{align*}

or
   
.. math::
   :nowrap:

   \begin{align*}
    y &  = \exp(1.11686 \log x -2.42759)
   \end{align*}

which is plotted in the following figure:


.. tikz::

   \begin{tikzpicture}[xscale=0.2]
   \draw[help lines] (0,0) grid[xstep=10]  (50,5);
   \draw[->, thick] (-0.5,0) -- node[below=0.5] {body mass} (51,0);
   \draw[->, thick] (0,-0.5) -- node[left=0.8,rotate=90] {brain mass}(0,5.5);
   \foreach \x in {5,10,15,20,25,30,35,40,45} {
   \draw (\x,-0.1) node[below]{\small{\x}} -- (\x,0.1);
   };
   \foreach \y in {1,2,3,4,5}
   {\draw (-0.5,\y) node[above,rotate=90] {\small{\y}} -- (0.5,\y);};
   \foreach \x / \y in {
   0.14/0.005,
   0.25/0.010,
   0.30/0.023,
   0.33/0.048,
   0.40/0.023,
   1.00/0.060,
   1.00/0.120,
   1.20/0.075,
   1.90/0.280,
   2.40/0.550,
   2.50/0.104,
   2.60/0.900,
   3.00/0.122,
   3.50/0.785,
   3.90/3.500,
   4.00/0.101,
   5.00/0.200,
   5.50/1.040,
   5.70/0.920,
   6.30/1.700,
   6.40/0.425,
   6.60/1.000,
   8.10/1.350,
   10.80/3.500,
   11.40/1.620,
   12.10/2.500,
   12.30/0.750,
   12.30/2.000,
   12.50/1.400,
   15.50/0.480,
   17.00/4.050,
   17.50/1.410,
   21.00/3.600,
   25.00/3.000,
   25.60/3.300,
   39.20/4.288,
   44.50/3.385,
   50.40/4.235   } \draw[blue] (\x,\y) ellipse(3pt and 0.5pt);
   \draw[domain=1:35,thick,color=green] plot
   (\x,{exp(1.11686*ln(\x)-2.42759)}) node{absolute}; 
   \end{tikzpicture}

===================
Rational Objectives
===================

In some applications the criterion to be minimized (or maximized)  is
given as the ratio between two linear or affine functions:

.. math::
   :nowrap:
      
   \begin{align*}
   \min \frac{\sum_j c_j \var{x}_j+a} {\sum_j d_j \var{x}_j+b} \\
   \sum_j A_{ij} \var{x}_j & \leq b_i & i = 1, \ldots, m  \\
   \var{x}_j & \geq0
   \end{align*}
   
In what follows, for simplicity, we will analyze the case
:math:`a=b=0`, i.e., the ratio of linear functions. The extension to
the general case is straightforward.
   
In this context special care has to be taken to avoid that, within the
feasible set, a solution exists in which the denominator is zero.
Frequently it is assumed that the denominator has a constant sign
(usually positive). When it is not a priori known whether the sign of
the denominator is constant, a simple auxiliary optimization problem
might be solved:

.. math::
   :nowrap:
      
   \begin{align*}
   \min \sum_j d_j \var{x}_j \\
   \sum_j A_{ij} \var{x}_j & \leq b_i & i = 1, \ldots, m  \\
   \var{x}_j & \geq0
   \end{align*}
   
If the optimal solution value of this problem is positive, than we can
conclude that the denominator is positive in the feasible set; if it
is zero, the problem cannot be solved, as there exists a feasible
solution with zero denominator in the objective function. If this
optimal value is negative, by solving a similar, maximization, problem
we might check whether the sign of the denominator is always negative.

In what follows we will assume a positive denominator.
Although this problem is non linear, thanks to a transformation
suggested by Charnes and Cooper in 1978,  :ref:`Charnes78` we can,
under some assumptions, transform it into an equivalent linear
optimization one.

The objective function, being  a fraction, will not change if we
multiply both the numerator and the denominator by the same non zero
quantity :math:`\var{q}`:

.. math::
   :nowrap:
      
   \begin{align*}
   \min \frac{\sum_j c_j \var{x}_j} {\sum_j d_j \var{x}_j} \cdot
   \frac{\var{q}} {\var{q}} &  \\
   \sum_j A_{ij} \var{x}_j & \leq b_i & i = 1, \ldots, m \\
   \var{x}_j & \geq0 
   \end{align*}
   
The (variable) quantity  :math:`\var{q}` can always be chosen in such a way as to
make the denominator equal to 1:

.. math::
   :nowrap:
      
   \begin{align*}
   \min \sum_j c_j \var{x}_j \var{q} & \\
   \sum_j d_j \var{x}_j \var{q} & = 1 \\
   \sum_j A_{ij} \var{x}_j & \leq b_i & i = 1, \ldots, m \\
   \var{x}_j & \geq0.
   \end{align*}
 
Thanks to the assumption on the sign of
the denominator, variable :math:`\var{q}` will  be strictly
positive. Thus, it is possible to multiply both sides of each
constraint in the model:

.. math::
   :nowrap:
      
   \begin{align*}
   \min \sum_j c_j \var{x}_j \var{q} & \\
   \sum_j d_j \var{x}_j \var{q} & = 1 \\
   \sum_j A_{ij} \var{x}_j \var{q} & \leq b_i \var{q} & i = 1, \ldots, m \\
   \var{x}_j \var{q} & \geq 0.
   \end{align*}
 
This model is still a non linear one, with bi-linear terms appearing
in many parts. We recall that, in general, bi-linear optimization is
very hard an optimization problem. However, in this special case, the
main variable of the original problem is everywhere multiplied by
:math:`q`, so that we can make a change of variable:
      
.. math::
   :nowrap:

   \[\var{y}_j := \var{x}_j\var{q}\]

and the model becomes  alinear one:

.. math::
   :nowrap:

   \begin{align*}
   \min \sum_j c_j \var{y}_j & \\
   \sum_j d_j \var{y}_j & = 1 \\
   \sum_j A_{ij} \var{y}_j - b_i \var{q} & \leq 0 & i = 1, \ldots, m \\
   \var{y}_j & \geq 0.
   \end{align*}
 
This is a linear problem in the variables :math:`\var{y} \in \R^n`
and :math:`\var{q}`. 
It has been proven  (see, for example, :cite:`Bazara93`), that if
the feasible set is *limited* (that is, if it is possible to limit from
above and below all variables) then
in an optimal solution to this linear optimization problem  variable
:math:`\var{q}` will be nonzero. Once the linear problem is
solved, therefore, it will be possible to recover the optimal solution of the
original problem through :math:`\var{x}_j = \var{y}_j / \var{q}`.

Problems of this kind arise in several contexts, from economics to
telecommunications. As a simple example we might consider production
planning problems in which, instead of minimizing the cost we minimize
the cost per product, i.e., the ratio between total cost and total
quantity produced. But the best known application of this fractional
optimization problem is related to efficiency evaluation, and is the
subject of the next section.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Comparative efficiency evaluation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We momentarily abandon the optimization framework to describe a very
important managerial problem. Eventually we will see how this is
turned into an optimization problem which fits the rational objective
function model just presented.

Consider the situation in which several "units" have to be compared in
terms of "efficiency". Units might be production plants, different
wards in hospitals, research departments, ...  
The task is to compare these units in order to put into evidence those
which are, in a sense to be made more precise inefficient in
comparison to the other ones. It is thus a peer-to-peer efficiency
evaluation, in which we are not certifying (absolute) efficiency but
just relative inefficiency. Consider, as a pedagogical example, the
following data concerning some healthcare-related statistics in some regions:


.. tabs::
   
   .. group-tab:: AMPL
      .. code-block:: ampl
      .. literalinclude:: AMPL/healthcare.dat
	 :caption: healthcare.dat
      
   .. group-tab:: Julia
      .. code-block:: Julia
      .. literalinclude:: Julia/TBD.jl
	 :caption: healthcare.dat

As we can see, each unit (here corresponding to a geographical region
in Italy) is represented through a set of
statistics. In the example, the available data is partitioned into
some inputs (the number of available medicine doctors, the number of
available beds) and outputs (number of home care assistance events,
number of patients per medicine doctor, number of recipes issued).
All these data is refereed to a standard of population in order to
make the data less sensitive to the population in each region. Please
take into account that the aim of this example is purely
illustrative, by no means it is intended as a realistic comparison,
even if data has been obtained fro official statistics. The choice of
inputs and outputs is quite arbitrary ad does not consider many
aspects which should be taken into account in a realistic scenario.

Inputs are associated to resources, while outputs are,
in a sense, "products". As a general rule, inputs are performance
parameters that ,in order to be labeled as efficient, we would like to
keep as low as possible. On the contrary, outputs are production
measurements and, as such, we would like to have these as large as
possible. Again, recall the scope of the example is illustrative. It
might be quite  questionable the label output associated to te number
of recipes: it might very well be the case that an efficient healthcare system issues less
recipes than an inefficient one!
Sometimes there might be some ambiguity in this classification and, in
fact, in some applications the same element is considered both as an
input and as an output. An example of this situation might be,e.g.,
the yearly budget: it surely is a 
resource, but it can be also a product, as in a competitive
environment being able to get a large budget is a result of efficiency.

A comparison among the different units might be very easy when there
is a single input and a single output.
In such a case a reasonable  efficiency measure would be the ratio between
the output and the input. From this ratio a ranking might be
generated, the most efficient  units being those for which the
output / input ratio is maximum. This ratio represents the total
production per unit resource. If we applied this criterion to the
health care data above, considering just the number of home assistance
cases and the
number of medicine doctors, unit U09 would result as the most efficient
one, with a ratio of more than 40, as opposed to, say,
units U18 with 1.80  cases per doctor or U02 with 2.3.

It is clear however that this single ratio cannot capture the
multi-objective nature of the comparison. Units are different and,
while one might excel in the home care, another one
might be instead specialized in a different kind of "output", and a comparison
based  simply on home care  would be too penalizing.

When, as is usually the case, every unit is characterized by a number
of different input and output data, a possibility for the
efficiency index would be to define a suitable ratio between a linear
combination of the outputs and a linear combination of the inputs.
Let

.. math::
   :nowrap:
      
   \[
   I_{1}^{u}, I_{2}^{u}, \ldots, I_{h}^{u} \quad
   O_{1}^{u}, O_{2}^{u}, \ldots, O_{k}^{u}
   \]
   
the input and output levels of unit
:math:`u`. An efficiency index can be defined for this unit by choosing
the weights :math:`\mathvar{\pi}` and :math:`\mathvar{\lambda}`
to form a linear combination of the inputs and
the outputs respectively:

.. math::
   :nowrap:
      
   \[
   \var{Eff}_{u}(\mathvar{\pi},\mathvar{\lambda}) =
   \frac{\sum_{i = 1}^{k} \mathvar{\lambda}_{i} O_{i}^{u}}{\sum_{i =
   1}^{h} \mathvar{\pi}_{i} I_{i}^{u}} 
   \]
 
As it is emphasized in the above definition, the efficiency index 
is a function of the coefficients  used to weight both inputs and outputs.
The problem of choosing the weights is indeed  a complex one, as every
choice leads to differences in treatment of one unit with respect to
others.

An interesting possibility, introduced by Charnes and Cooper and known
under the name of "Data Envelopment Analysis" :index:`Data Envelopment
Analysis` (or :index:`DEA`), consists in letting every unit choose the
most convenient coefficients.
That is, every unit in turn is asked to decide the weights; of course
each unit will try to choose the coefficients which maximimize its own
index. However, with the same weights, all the other units need also to be
evaluated and their efficiency score computed. If, even with this
freedom of choice, a unit cannot find a set of weights which allows it
to excel over all the others, than that unit will be labeled
as inefficient.

As all efficiency indices are given as a ratio of linear combinations
of inputs and outputs, we can always constraint the resulting fraction
to be bounded above by 1 (which corresponds to 110% efficiency).
By introducing a constraint that limits all
output/input ratios  to the range :math:`[0,1]` we will define as
"inefficient" a unit that cannot decide a set of weights which let it
reach  its output/input  ratio 1 while keeping limited by 1 the
indices of all the remaining units, evaluated with the same weight factors.

From a modeling point of view,
the evaluation of the most convenient index corresponds to the
solution of a set of optimization problems, one for each unit, of the
form:

.. math::
   :nowrap:
      
   \begin{align*}
   \var{eff}^\star_{u} = &
   \max \frac{\sum_{i = 1}^{k} \mathvar{\lambda}_{i} O_{i}^{u}} {\sum_{i = 1}^{h}
   \mathvar{\pi}_{i} I_{i}^{u}} \\
   \frac{\sum_{i = 1}^{k} \mathvar{\lambda}_{i} O_{i}^{s}}{\sum_{i =
   1}^{h} \mathvar{\pi}_{i} I_{i}^{s}} 
   &
   \leq 1 & \forall \, s \in \set{Units}  \\
   \mathvar{\lambda}, \mathvar{\pi} & \geq 0
   \end{align*}
 
The objective is to maximize
the efficiency indicator for  unit :math:`u`,
with the constraints preventing the
efficiency indices of other units to exceed 100%.

This non linear problem can be easily transformed into an equivalent
linear one, as we have seen previously. First, the denominator has
positive sign, being a non negative combination of positive numbers.
Actually the
denominator might be zero, when the coefficients of the non zero
inputs of the current unit are chosen to be zero. If this might
happen, a small positive constant might be added to the lower bound on
each variable. 
The transformation of this problem as an equivalent  linear one can be
easily obtained. Here not only the objective is fractional, but also
the constraints.
However, it is trivial to transform the constraints into linear
equivalent ones, multiplying both sides by the denominator.
Following the transformation given previously, we reduce the problem
into one of maximizing the output indicator of the current unit,
sbuject to a nornalization constraint on its input index:

.. math::
   :nowrap:
      
   \begin{align*}
   \max \sum_{i=1}^{k} O_{i}^{u} \var{x}_{i} \\
   \sum_{i = 1}^{h} I_{i}^{u} \var{y}_{i} & = 1 \\
   \sum_{i = 1}^{k} O_{i}^{s} \var{x}_{i} - \sum_{i = 1}^{h} I_{i}^{s} \var{y}_{i}
   & \leq0 &    \forall \, s  \in \set{Units} \\
   \var{x}, \var{y} & \geq 0
   \end{align*}

   
It can be observed that, differently from the general case, 
the use of the  auxiliary variable :math:`\var{q}` was not
required here, as the  right hand sides of all inequalities are zero.

An implementation of this model might be the following:

.. tabs::
   
   .. group-tab:: AMPL
      .. code-block:: ampl
      .. literalinclude:: AMPL/dea.mod
	 :caption: dea.mod
      
   .. group-tab:: Julia
      .. code-block:: Julia
      .. literalinclude:: Julia/TBD.jl
	 :caption: dea.jl

The model can now be run, looping over all possible units to find the
maximal efficiency achievable. An implementation of the commands to
run all these optimization and prepare an output statistics is the
following:


.. tabs::
   
   .. group-tab:: AMPL
      .. code-block:: ampl
      .. literalinclude:: AMPL/dea.run
	 :caption: dea.run
      
   .. group-tab:: Julia
      .. code-block:: Julia
      .. literalinclude:: Julia/TBD.jl
	 :caption: dea_run.jl

and, with the data presented before, a fraction of the generated  output is the following:

.. code-block:: ampl
.. literalinclude:: AMPL/dea.out
   :caption: dea.out
   :lines: 1-20

It can be seen that with these data, the following maximal efficiency
can be obtained (in the table we report the optimal weights found by
each unit):

.. math::
   :nowrap:

   \begin{align*}
   \begin{array}{l|r|rrrrr}
   Unit & eff & Doctors & beds & HomeAss & Patients & recipes \\ \hline
   U01 & 0.984 & 0.0139 & 0 & 0 & 0.000179 & 0.00768\\
   U02 & 1 & 0.0122 & 0.000772 & 0 & 0.000832 & 0\\
   U03 & 0.941 & 0.0152 & 0 & 0 & 0.000196 & 0.00838\\
   U04 & 1 & 0.0185 & 0 & 0 & 0.000239 & 0.0102\\
   U05 & 0.91 & 0.0147 & 0 & 0 & 0.00019 & 0.00814\\
   U06 & 0.941 & 0.0149 & 0 & 0 & 0.000193 & 0.00826\\
   U07 & 0.823 & 0.013 & 0 & 0 & 0.000168 & 0.00718\\
   U08 & 0.945 & 0.013 & 0 & 0 & 0.000168 & 0.00718\\
   U09 & 1 & 0.0143 & 0 & 0 & 0.000185 & 0.0079\\
   U10 & 1 & 0.00244 & 0.00168 & 0.000356 & 0 & 0\\
   U11 & 0.719 & 0.00907 & 0.000521 & 0 & 0.000582 & 0\\
   U12 & 0.863 & 0.0105 & 0.000604 & 0.0001 & 0.000675 & 0\\
   U13 & 0.935 & 0.0111 & 0.000533 & 0.000118 & 0 & 0.00739\\
   U14 & 0.996 & 0.0106 & 0.00051 & 0.000113 & 0 & 0.00707\\
   U15 & 1 & 0 & 0.0125 & 0.00104 & 0 & 0\\
   U16 & 1 & 0 & 0.0147 & 0 & 0.000881 & 0\\
   U17 & 0.949 & 0.0125 & 0 & 0 & 0 & 0.00855\\
   U18 & 0.913 & 0.01 & 0.00129 & 0 & 0 & 0.00815\\
   U19 & 0.993 & 0.0122 & 0 & 0 & 0 & 0.00834\\
   U20 & 1 & 0.0108 & 0.00139 & 0 & 0 & 0.00877\\
   U21 & 1 & 0.0108 & 0.00139 & 0 & 0 & 0.00877\\ \hline
   \end{array}
   \end{align*}

From the result above, it van be seen that a few units are labeled as
not efficient. As an example, unit U11 has an efficiency score of only
0.719. This means that with the best possible choice of weights, this
unit cannot reach higher efficiency score, as other units, which we
call competitors, reach their maximum index 1 with that choice. For
unit U11 the competitors, i.e. those units which display 100%
efficiency when evaluated with the weights which are the most
convenient for U11, turn out to be: U04, U09, U16. From 
weights values we can observe the relative importance of inputs and
outputs. Of course, as the efficiency index is a ratio of linear
expressions, it does not change multiplying every weight by  positive
constant. Thus it is more appropriate to consider relative weights (or
ratio between two weights).  
   
This comparative efficiency  model is the simplest one; there exist
many variations of the basic model which can give different
evaluations. The model above prescribes maximizing the output/input
ratio. It is easy to write a model in which instead, we minimize the
input/output ratio; perhaps quite surprisingly, this input-oriented
model can produce different efficiency rankings.
In all models, 
further constraints might be introduced to limit the choice of
weights. It is possible, in fact, to prescribe that some weights are larger than others,
to reflect the relative importance of different input and output
categories). Some extensions try to take into account scale economies
or diseconomies by introducing some non linearity useful when
comparing unit  of very different sizes. We refer to the very abundant
literature on this subject for further details.


"""""""""""""""""""""""""""""""""""""""
Dual of the efficiency evaluation model
"""""""""""""""""""""""""""""""""""""""

It is not difficult, yet quite interesting, to analyze the dual of the
linear efficiency optimization model. In order to obtain a dual
representations, let us first slightly reformulate the problem:


.. math::
   :nowrap:

   \begin{align*}
   \sum_{i = 1}^{h} I_{i}^{u} \var{y}_{i} & = 1 \\
   \sum_{i = 1}^{k} O_{i}^{s} \var{x}_{i} - \sum_{i = 1}^{h} I_{i}^{s} \var{y}_{i}
   & \leq0 &
   \forall \, s  \in \set{Units} \\
   \var{x}, \var{y} & \geq0
   \end{align*}


In order to formulate the dual, we observe that this problem has two
groups of constraints: a single normalization constraint for the
current unit and a group of normalization constraints, one for each
unit. Let us associate a single dual variable
:math:`\mathvar{\theta}` to the normalization constraint of the
current unit, and the set of dual variables
:math:`\mathvar{\lambda}_s` for the second group.  The constraints
in the dual will be associated to the two groups of primal variables:
the input weights and the output weights. Following the standard dual
transformation for linear problems, we get:

.. math::
   :nowrap:
      
   \begin{align*}
   - \max \mathvar{\theta} & \\
   I_i^{u} \mathvar{\theta} - \sum_{s \in  \in \set{Units}} I_i^s
   \mathvar{\lambda}_s & \leq 0 & \forall \, i \in \set{Inputs} \\
   \sum_{s \in \set{Units}} O_i^s \mathvar{\lambda}_s & \leq -O_i^{u}
   & \forall \, i \in \set{Outputs} \\
   \mathvar{\lambda}_s & \leq 0 & \forall\,s \in \set{Units}
   \end{align*}

For convenience, let us substitute  each variable with the opposite
one. We obtain:

.. math::
   :nowrap:
      
   \begin{align*}
   \min \mathvar{\theta} &  \\
   \sum_{s  \in \set{Units}} I_i^s
   \mathvar{\lambda}_s & \leq I_i^{u} \mathvar{\theta} &
   \forall \, i \in \set{Inputs} \\
   \sum_{s  \in \set{Units}} O_i^s \mathvar{\lambda}_s & \geq O_i^{u}
   & \forall \, i \in \set{Outputs} \\
   \mathvar{\lambda}_s & \geq 0  & \forall\, s  \in \set{Units}
   \end{align*}
 
From  duality theory we know that the optimum of
this problem will be equal to that of the primal and  will always be
between 0 and  1. Even without using duality,
it is easy to see that the optimal value of
:math:`\mathvar{\theta}` cannot  exceed 1, since  a fesible solution
of value  1  can always be found by choosing  :math:`\mathvar{\lambda}_{u}=1`  and all  other components of
:math:`\mathvar{\lambda}` to zero.
      
The dual model lends itself to an important interpretation.
From the left hand side of the constraints we see that, for every input or output,
a linear combination of units  is composed.

We may think that we are building a "virtual" unit, not existing in
real life, obtained by combining with non negative coefficients the
data of the available units. If, besides the request of non
negativity, we added a normalization constraint asking for the weights
to sum to 1 at most, then this operation would lead to the convex
envelop (or convex combination) of all units in the input/output
feature space. From this geometrical interpretation, the efficiency
evaluation model gets the name "Data Envelopment Analysis" or DEA
:index:`DEA`. However we do not require the normalization constraint.

This virtual unit composed of a mixture of real ones, including the
one we are currently evaluating, :math:`u`, for a generic output
feature :math:`i`, has a virtual output equal to

.. math::
   :nowrap:
      
   \begin{align*}
   \sum_{s  \in \set{Units}} O_i^s \mathvar{\lambda}_s &
   & \forall \, i \in \set{Outputs}
   \end{align*}

The associated constraint asks for this virtual output to be at least
as good as the real one, :math:`O_i^{u}`. Notice that, within the
units which can be used in order to form the virtual one, there is
also unit :math:`u`. Thus it is very easy to satisfy these
constraints. However, the analogous constraint on the virtual value
associated to inputs has a right hand side which is :math:`I_i^{u}
\mathvar{\theta}`.
Thus, choosing :math:`\mathvar{\theta} =1`, which is a feasible
choice, we always can satisfy all of the constraints. However, thanks
to the objective function, we are interested in checking whether a
linear combination of the units, different from the trivial one,
exists which allows to obtain a value  :math:`\mathvar{\theta} <1`.
If this is possible, then the current unit will be labeled as not
efficient, as there exists a virtual unit (a unit in the envelop of
the others) which produces at least as much output but with strictly
less resources (no more than  :math:`\mathvar{\theta}` times the
resources of :math:`u`).

Thus this dual model allows us to see the efficiency evaluation
problem from a different, complementary, point of view: if and only if
a unit is inefficient, we can find a set of competitors which,
suitably combined, can produce enough output with lower input
levels. These are the units from which the inefficient one should
learn how to improve. They are a benchmark for this unit.
   
In the example, looking at the dual values when optimizing the index
of unit U11, we obtain the weights:

.. code-block:: ampl

   U04: 0.113482
   U09: 0.336625
   U16: 0.399953   

Multiplying the input and output data by these coefficients we find
that the virtual unit has the following virtual data (in parentheses
the corresponding data for unit U11):

.. code-block:: ampl

   Patients 1053     (1053)
   HomeAss  1222     (1222)
   recipes    43.24  (12)

   Doctors    59.69  (83)
   beds       340.87 (474)
   
from which it can be seen that the virtual unit produces at least as
much as unit 11, but with far less inputs (actually, 71.9%
less). Thus, this can be considered as a proof of inefficiency.

We can also observe that, thanks to complementarity, the units which
will enter this virtual unit are units with efficiency score equal
to 1. And, following the same line of reasoning, the outputs for which
the virtual unit produces strictly more than the currently evaluated
one, are weighted as zero in the primal efficiency optimization
model. And, similarly, the inputs which are required in a fraction
which is strictly smaller than :math:`\mathvar{\theta}` have a zero
weight in the primal problem.
In the example, the weights for unit U11 are:

.. code-block:: ampl

   weights:

   Patients: 0.000582
   HomeAss: 0.000087
   recipes: 0
   
   Doctors: 0.009073
   beds: 0.000521

and of course a similar reasoning can be extended to all other units.

.. include:: closing.rst

